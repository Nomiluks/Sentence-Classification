{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Training Set:  7000\n",
      "Size of Training Labels:  7000\n",
      "Size of Test Set:  2977\n",
      "Size of Test Labels:  2977\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import w2v_utils  as w2v_utils\n",
    "import numpy as np\n",
    "import gensim.models\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import random\n",
    "from __future__ import division\n",
    "import math \n",
    "import re\n",
    "\n",
    "def prepareData(path,trainExamples):\n",
    "    \n",
    "    trainData=[]\n",
    "    fp=open(path, 'rb')\n",
    "    data=fp.readline()\n",
    "    data=fp.readline()\n",
    "    while data!=\"\":\n",
    "        trainData.append(data.split('##'))\n",
    "        data=fp.readline()\n",
    "    data=[]\n",
    "    labels=[] \n",
    "    lb=[]\n",
    "    random.shuffle(trainData)\n",
    "    for record in trainData:\n",
    "        data.append(record[0])\n",
    "        lb.append(record[1].split('\\t'))\n",
    "        labels.append(lb[0][0])\n",
    "        lb=[]\n",
    "    train_set, test_set, train_label, test_label = data[:trainExamples], data[trainExamples:],labels[:trainExamples],labels[trainExamples:]\n",
    "    \n",
    "        \n",
    "    return [train_set, test_set, train_label, test_label]\n",
    "    \n",
    "    \n",
    "train_set, test_set, train_labels, test_labels=prepareData('Case Advanced Find View (1).txt',7000)\n",
    "print 'Size of Training Set: ',len(train_set)\n",
    "print 'Size of Training Labels: ',len(train_labels)\n",
    "print 'Size of Test Set: ',len(test_set)\n",
    "print 'Size of Test Labels: ',len(test_labels)\n",
    "\n",
    "trainingSentences = [w2v_utils.review_to_wordlist(review,True) for review in train_set]\n",
    "testSentences     = [w2v_utils.review_to_wordlist(review,True) for review in test_set]\n",
    "\n",
    "\n",
    "word2vecInput = trainingSentences+testSentences\n",
    "\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "'''\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 5    # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10           # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "\n",
    "print \"Training model...\"\n",
    "model = gensim.models.Word2Vec(workers=num_workers, \n",
    "                          size=num_features, min_count = min_word_count, \n",
    "                          window = context, sample = downsampling)\n",
    "\n",
    "model.build_vocab(word2vecInput)#build vocablary from all training words\n",
    "model.train(word2vecInput)\n",
    "model.save('word2vec')\n",
    "print \"Finish training model...\"\n",
    "'''\n",
    "model = Word2Vec.load('word2vec')\n",
    "# preparing  data\n",
    "x_train,Y_train,x_val,Y_val =  w2v_utils.splitDataset(train_set,train_labels, split=0.999) \n",
    "x_test  = test_set\n",
    "Y_test  = test_labels\n",
    "x_train = w2v_utils.clean(x_train, True)\n",
    "x_val = w2v_utils.clean(x_val,True)\n",
    "x_test = w2v_utils.clean(x_test,True)\n",
    "\n",
    "Y_train = [re.sub('[\"]', '', label) for label in Y_train]\n",
    "Y_val   = [re.sub('[\"]', '', label) for label in Y_val]\n",
    "Y_test  = [re.sub('[\"]', '', label) for label in Y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process of word to vector conversion initiated!\n",
      "Training set Sentences word vector dimensions: \n",
      "(6993, 300)\n",
      "Validation set Sentences word vector dimensions: \n",
      "(7, 300)\n",
      "Gold Sentences word vector dimensions: \n",
      "(2977, 300)\n",
      "Training Data:  (6993, 300) (6993,)\n",
      "Validation Data:  (7, 300) (7,)\n",
      "Gold Test Data:  (2977, 300) (2977,)\n"
     ]
    }
   ],
   "source": [
    "######## getting sentence vectors\n",
    "print \"Process of word to vector conversion initiated!\"\n",
    "#Build word vector for training set by using the average value of all word vectors in the tweet, then scale\n",
    "def buildWordVector(text, size, debug = False):        \n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in text:\n",
    "        try:\n",
    "            wordVector = model[word].reshape((1, size))\n",
    "            vec += wordVector\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            if debug:\n",
    "                print \"not found! \",  word\n",
    "                count += 1.\n",
    "            continue\n",
    "    if count != 0:     \n",
    "        #Sentence vectors\n",
    "        vec_avg    = vec/count   \n",
    "    return vec\n",
    "\n",
    "######## Converting training, validation and test sentences into vectors\n",
    "x_train = np.concatenate([buildWordVector(sentence, model.layer1_size, debug = False) for sentence in x_train])\n",
    "print \"Training set Sentences word vector dimensions: \\n\",x_train.shape\n",
    "\n",
    "x_val = np.concatenate([buildWordVector(sentence, model.layer1_size, debug = False) for sentence in x_val]) \n",
    "print \"Validation set Sentences word vector dimensions: \\n\",x_val.shape\n",
    "    \n",
    "x_test = np.concatenate([buildWordVector(sentence, model.layer1_size, debug = False) for sentence in x_test])\n",
    "print \"Gold Sentences word vector dimensions: \\n\",x_test.shape\n",
    "\n",
    "\n",
    "\n",
    "############################### Preparing Data with One Hot #######################################\n",
    "X_train = x_train #training Data\n",
    "y_train = w2v_utils.oneHotVectors(Y_train)\n",
    "#y_train = w2v_utils.categoriesToLabels(Y_train)\n",
    "y_train = np.nonzero(y_train)[1]\n",
    "print \"Training Data: \",X_train.shape, y_train.shape\n",
    "\n",
    "###############################Preparing Validation Data\n",
    "X_val = x_val\n",
    "y_val = w2v_utils.oneHotVectors(Y_val)\n",
    "#y_val = w2v_utils.categoriesToLabels(Y_val)\n",
    "y_val   = np.nonzero(y_val)[1]\n",
    "print \"Validation Data: \",X_val.shape, y_val.shape\n",
    "\n",
    "###############################Preparing Test Gold Data\n",
    "X_test = x_test\n",
    "y_test = w2v_utils.oneHotVectors(Y_test)\n",
    "#y_test = w2v_utils.categoriesToLabels(Y_test)\n",
    "y_test  = np.nonzero(y_test)[1]\n",
    "\n",
    "print \"Gold Test Data: \", X_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------\n",
      "Confusion Matrix: \n",
      "\n",
      "Predicted     0    1   All\n",
      "Actual                    \n",
      "0          1567  652  2219\n",
      "1           440  318   758\n",
      "All        2007  970  2977\n",
      "--------------------------------------------------------------\n",
      "Classification  Report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   Research       0.71      0.78      0.74      2007\n",
      "        Bug       0.42      0.33      0.37       970\n",
      "\n",
      "avg / total       0.61      0.63      0.62      2977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "#correctPred = (y_test != y_pred).sum()\n",
    "#print \"Total test instances: \",len(y_test),\"Total Correctly Predict classes: \",correctPred\n",
    "\n",
    "print \"--------------------------------------------------------------\"\n",
    "print \"Confusion Matrix: \\n\\n\",\n",
    "print w2v_utils.confusionMatrix(y_test, y_pred)\n",
    "\n",
    "print \"--------------------------------------------------------------\"\n",
    "print \"Classification  Report: \\n\\n\",\n",
    "target_names = ['Research', 'Bug']\n",
    "print(classification_report( y_test,y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='linear', C = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------\n",
      "Confusion Matrix: \n",
      "\n",
      "Predicted     0    1   All\n",
      "Actual                    \n",
      "0          1948  868  2816\n",
      "1            59  102   161\n",
      "All        2007  970  2977\n",
      "--------------------------------------------------------------\n",
      "Classification  Report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   Research       0.69      0.97      0.81      2007\n",
      "        Bug       0.63      0.11      0.18       970\n",
      "\n",
      "avg / total       0.67      0.69      0.60      2977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print \"--------------------------------------------------------------\"\n",
    "print \"Confusion Matrix: \\n\\n\",\n",
    "print w2v_utils.confusionMatrix(y_test, y_pred)\n",
    "\n",
    "print \"--------------------------------------------------------------\"\n",
    "print \"Classification  Report: \\n\\n\",\n",
    "target_names = ['Research', 'Bug']\n",
    "print(classification_report( y_test,y_pred, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
